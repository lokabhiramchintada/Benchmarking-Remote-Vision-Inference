# Inference Latency Test

This project measures **end-to-end inference latency** for a YOLO object detection model running on a remote server. The client uploads images, receives detection results, and records:

* Upload latency
* Inference latency
* Download latency
* Total round-trip latency

Results are saved to a CSV file for comparison.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ client.py
â”œâ”€â”€ server.py
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ test1.jpg
â”‚   â”œâ”€â”€ test2.jpg
â”‚   â””â”€â”€ test3.jpg
â””â”€â”€ yolo12m.pt
```

---

## ğŸ–¥ï¸ Remote Access (Client â†’ Server Tunnel)

Create an SSH tunnel so your local client can access the remote FastAPI server:

```bash
ssh -L 9000:localhost:8000 -J your_username@ada.iiit.ac.in your_username@gnode046
```

This forwards:

* Local port **9000**
* To remote **localhost:8000**

---

## ğŸš€ Start the Server

Run the FastAPI server on the remote machine:

```bash
uvicorn server:app --host 0.0.0.0 --port 8000
```

---

## ğŸ“¦ Install Requirements

### Client

```bash
pip install requests pandas
```

### Server

```bash
pip install fastapi uvicorn ultralytics opencv-python numpy
```

---

## ğŸ§  YOLO Model

The server loads:

```
yolo12m.pt
```

Place this file in the same directory as `server.py`.

---

## ğŸ“¡ Client Script (Latency Measurement)

The client uploads three images and logs latency metrics:

* Upload time
* Inference time (server-measured)
* Download time
* Total end-to-end time
* Number of detections

Results print as a table and save to:

```
latency_results.csv
```

---

## ğŸ“Š Output Example

```
===== FINAL COMPARISON =====

        Image     Description  Upload (ms)  Inference (ms)  Download (ms)  Total (ms)  Num Detections
   test1.jpg  Low detections         55.79           59.74          55.79      171.32              22
```

---

## ğŸ§® Latency Calculation Method

| Metric            | Source                  |
| ----------------- | ----------------------- |
| Total             | Client round-trip time  |
| Inference         | Server timestamps       |
| Upload + Download | Total âˆ’ Inference       |
| Upload            | (Total âˆ’ Inference) Ã· 2 |
| Download          | (Total âˆ’ Inference) Ã· 2 |

This assumes symmetric network latency.

---

## ğŸ› ï¸ API Endpoint

`POST /infer`

Returns JSON:

```json
{
  "server_receive_time": ...,
  "infer_start_time": ...,
  "infer_end_time": ...,
  "detections": [...]
}
```

---

## ğŸ“Œ Notes

* Run server first
* Ensure SSH tunnel is active
* Client uses `http://localhost:9000/infer`
* Use Python 3.8+

---

## âœ”ï¸ Use Case

This setup is ideal for:

âœ… Edge-cloud latency benchmarking
âœ… Model deployment evaluation
âœ… Network performance analysis
âœ… Comparing workloads with different detection counts

---

